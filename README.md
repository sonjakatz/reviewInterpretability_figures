# Designing Interpretable Deep Learning Applications in Genomics: a Quantitative Analysis


## Abstract

Deep learning applications have had a profound impact in many scientific fields, including genomics. Deep learning models can learn complex interactions between and within omics data, however interpreting and explaining these complex  models can be challenging. Interpretability is not only essential to help progress our understanding of the biological mechanisms underlying traits and diseases but also for establishing trust in these models' efficacy for healthcare applications. Recognizing this importance, the recent years has seen the development of numerous diverse interpretability strategies, making it increasingly difficult to navigate the field. In this review, we present a quantitative analysis on the challenges in designing interpretable deep learning solutions in genomics. We analyze design challenges related to the characteristics of genomics data, the neural network architectures applied, and strategies for interpretation. By quantifying the current state of the field with a predefined set of criteria, we find the most frequent solutions, highlight exceptional examples, and identify unexplored opportunities for developing interpretable deep learning models in genomics. 


## Publication

This repository contains all the scripts used during the generation of figures for our publication published in [bioRXiv](https://www.biorxiv.org/)

The data utilised can be found in an interactive format as [Notion table](https://shorturl.at/juRX9) - you are invited to go and explore yourself!

 
## Contact 

Please feel free to reach out to us through either of the following emails if you have any questions or need any additional files:

<a.vanhilten@erasmusmc.nl>
<sonja.katz@wur.nl>